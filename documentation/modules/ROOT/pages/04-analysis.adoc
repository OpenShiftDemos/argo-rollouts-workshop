= Analysis
include::_attributes.adoc[]

In this module we will:

* Review and understand the concept of an Analysis
* Deploy a Blue-Green rollout with an Analysis
* Perform a successful promotion of an image tested by an Analysis
* Perform and observe a promotion that fails an Analysis

[#analysis-overview]
== Analysis Overview

When performing upgrades of services there is a need to test the new version
that is being deployed to ensure functionality is not being negatively impacted. The
link:https://argo-rollouts.readthedocs.io/en/stable/features/analysis[Analysis,window='_blank'] feature
enables Rollouts to collect data and metrics from a variety of providers to validate the
new version of the application.

In addition to collecting data, an Analysis can include a Job to drive more advanced use cases. For example
a Job could be used to run load against an application in order to generate the metrics needed to
validate the revision.

To deploy an Analysis, first an AnalysisTemplate is created that defines the analysis that is required
and then is associated with one or more rollouts. The way an analysis is associated with a rollout is dependent on
the rollout strategy used. In this module we will look at it from the perspective of the Blue-Green
strategy we have deployed previously however in the next module we delve into it for the Canary strategy as well.

In the blue-green strategy, an Analysis can be added as either pre-promotion or post-promotion. Pre-promotion
is used before the new version is deployed and is useful for validating the deployment prior to cutting over to it
for live traffic. Post-promotion is executed after the cut-over and can validate that the deployment is working
with live traffic.

[#analysis-deployment]
== Analysis Deployment

In this section we will deploy the same Blue-Green rollout we did previously but with
a pre-promotion analysis included along with the corresponding analysis template. Prior to starting,
confirm you are still at the correct path.

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
cd ~/argo-rollouts-workshop/documentation/modules/ROOT/examples/
----

Next, let's explore the manifests that we will be deploying in the `./bluegreen-analysis/base` folder:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ls ./bluegreen-analysis/base
----

Here you will see the same files that we used previously, however there is a new file `analysistemplate.yaml`. Examining
the file we see it appears as follows:

.link:https://github.com/OpenShiftDemos/argo-rollouts-workshop/blob/main/documentation/modules/ROOT/examples/bluegreen-analysis/base/analysistemplate.yaml[./bluegreen-analysis/base/analysistemplate.yaml,window='_blank']
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$bluegreen-analysis/base/analysistemplate.yaml[]
----

This AnalysisTemplate is broken up into two broad sections as follows:

* `.spec.args`. This is where you can specify arguments which are passed by the rollout when using the template. These arguments
enable the template to be reused across many different rollouts.
* `.spec.metrics`. These are the metric providers that will be used to collect data for the analysis as well as any jobs
that need to be executed.

In the metrics provider section we can see we have two providers defined, one that uses the web metric provider to pull
metrics from Thanos, an aggregator for Prometheus data, and a job that runs link:https://github.com/JoeDog/siege[Apache Siege,window='_blank']
to drive some load on the application.

[NOTE]
We are using the web metric provider here since the Argo Rollouts v1.2 that we are using does not
support authentication with the Prometheus provider. This capability has been recently added in Rollouts as
of v1.6.0.

The `count` and `interval` fields in the `success-rate` metric powered by the web provider indicate that the metric will be checked four times
with a thirty second interval between each check. The `failureLimit` determines how many failures are permitted for the rollout to be considered
a success, here we set a failure limit of 0.

Finally, remember we are running this Analysis in the pre-promotion phase of the blue-green strategy so the application will not
be receiving load from users, therefore generating load with Apache Siege will generate the metrics we need to determine whether
to proceed with the promotion.

In the arguments we are taking two key arguments, `query` and `route-url`, which will be passed from the
Rollout. Notice that the `query` argument is being used at the end of the URL of the web provider as a
query parameter, this is the Prometheus query that we want to execute against the OpenShift monitoring stack.

[subs="quotes"]
----
provider:
  web:
    url: https://thanos-querier.openshift-monitoring:9091/api/v1/query?query={{ *args.query* }}
----

The `route-url` parameter is being used in the second provider, the job, where we will be creating load against. This
parameter is used to specify the OpenShift route URL where we want to drive the load, i.e the URL that
siege will be hitting when it generates load.

Finally a third parameter, `api-token`, is provided by a secret. This secret provides the token needed
to access the OpenShift monitoring stack, it was created by the GitOps process which provisioned this workshop.

Next let's look at the rollout and see how the AnalysisTemplate is wired into the rollout.

.link:https://github.com/OpenShiftDemos/argo-rollouts-workshop/blob/main/documentation/modules/ROOT/examples/bluegreen-analysis/base/rollout.yaml[./bluegreen-analysis/base/rollout.yaml,window='_blank']
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$bluegreen-analysis/base/rollout.yaml[]
----

Notice that in `.spec.strategy.blueGreen` we have now defined the `prePromotionAnalysis` field. In this field
we define the analysis template we want to use as well as the arguments that the rollout needs to provide the
template.

[NOTE]
The query parameter is URL encoded since we are using the Web provider rather then the Prometheus
provider hence the need to encode it appropriately.

Next to deploy this new version of the blue-green rollout, execute the following command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
kustomize build ./bluegreen-analysis/base | sed "s/%SUB_DOMAIN%/$SUB_DOMAIN/" | sed "s/%USER%/$USERNUM/" | oc apply -n user$USERNUM-prod -f -
----

Check that the rollout has deployed successfully in the Argo Rollouts dashboard, if you have
closed the browser tab with the dashboard remember the URL can be retrieved as follows:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get route -n user%USERNUM%-tools dashboard -o jsonpath='{"https://"}{.spec.host}{"\n"}'
----

The dashboard for our blue-green rollout will appear identically as follows:

image::argo-rollout-dashboard-bg-analysis-initial.png[]

Next confirm that the application is displaying blue squares by opening the active route URL in your browser:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get route -n user%USERNUM%-prod active -o jsonpath='{"https://"}{.spec.host}{"\n"}'
----

image::rollouts-demo-app-blue.png[]

[#analysis-promotion]
== Promotion with Analysis

In this section we will promote new images using the Analysis to test the new version of the
application. In the first part we will perform a promotion where the analysis succeeds and
the promotion occurs. In the second part we will have the application trigger errors causing
the analysis to fail and the promotion to be aborted.

Note that auto-promotion is enabled.

=== Analysis Passes

With the updated blue-green rollout deployed, let's run through a promotion to a new image where
the analysis succeeds and observe the behavior. In the OpenShift console, go to the Pipelines
and do a promotion to a `green` image. As a reminder this is located here in the console:

image::console-pipelines-overview.png[]

Wait for the pipelines to complete and then go to the Argo Rollouts dashboard where
you may see the following if the Analysis is still running where the Analysis button
is shown in grey (highlighted in the image with a red outline):

image::argo-rollouts-analysis-in-progress-overview.png[]

When a promotion of a rollout with an analysis is executed the controller will
generate an analysis run for each template execution. Since we only have one template
being used in pre-promotion we will only see one analysis run being created however
it is possible for a promotion to have multiple analysis runs dependent on the
rollout configuration, i.e. in blue-green defining both pre and post promotion analysis.

Clicking on the analysis button will expand the view to show the in-progress analysis
that is running:

image::argo-rollouts-analysis-in-progress-details.png[]

Note there are two green boxes with graph icons that are being shown in separate
rows. Each row represents a provider and if you recall we had two providers, the job running
Siege and the metric provider. Hovering the mouse over the buttons will display
additional information:

image::argo-rollouts-analysis-in-progress-hover.png[]

Once the analysis is complete the Analysis button will go green to show that it successfully
completed:

image::argo-rollouts-analysis-completed.png[]

Notice that one row has four graph icons, this represents the four measurements that were taken
during the execution of the AnalysisRun. Hovering the mouse over any of these graph icons will
show you the measurement result.

[NOTE]
You may need to refresh the browser to see new metrics being added to the AnalysisRun depending
on connectivity during the workshop.

You can also use the Argo Rollouts CLI to examine the status of the Rollout by executing the `get`
command that we used earlier:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc argo rollouts get rollout rollouts-demo -n user%USERNUM%-prod
----

This will display output similar to the following assuming the analysis run has completed executing:

[.console-output]
[source,bash,subs="attributes+,+macros"]
----
Name:            rollouts-demo
Namespace:       user%USERNUM%-prod
Status:          ✔ Healthy
Strategy:        BlueGreen
Images:          quay.io/openshiftdemos/rollouts-demo:green (stable, active)
Replicas:
  Desired:       2
  Current:       2
  Updated:       2
  Ready:         2
  Available:     2

NAME                                                        KIND         STATUS         AGE    INFO
⟳ rollouts-demo                                             Rollout      ✔ Healthy      6m58s
├──# revision:2
│  ├──⧉ rollouts-demo-5999df6cf9                            ReplicaSet   ✔ Healthy      2m23s  stable,active
│  │  ├──□ rollouts-demo-5999df6cf9-cw5bm                   Pod          ✔ Running      2m23s  ready:1/1
│  │  └──□ rollouts-demo-5999df6cf9-rfgzr                   Pod          ✔ Running      2m23s  ready:1/1
│  └──α rollouts-demo-5999df6cf9-2-pre                      AnalysisRun  ✔ Successful   2m20s  ✔ 5
│     └──⊞ 17525abd-96e7-4abe-bac3-e9f62249a353.run-load.1  Job          ✔ Successful   2m20s
└──# revision:1
   └──⧉ rollouts-demo-66d84bcd76                            ReplicaSet   • ScaledDown   6m58s
      ├──□ rollouts-demo-66d84bcd76-95xh8                   Pod          ◌ Terminating  6m58s  ready:1/1
      └──□ rollouts-demo-66d84bcd76-qq8lt                   Pod          ◌ Terminating  6m58s  ready:1/1
----

Note for the new revision the results of the AnalysisRun is shown with the INFO column
showing a checkmark with a 5. The five shows the aggregated total of four measurement
checks plus the job executing successfully for a total of five.

=== Analysis Fails

Now that we have experienced a successful promotion, let's examine the behavior when the
application has errors and the analysis fails. To start, open the preview version of the application in a separate
browser tab, remember we can retrieve the route for the preview service by using this command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get route -n user%USERNUM%-prod preview -o jsonpath='{"https://"}{.spec.host}{"\n"}'
----

Slide the error bar as shown below all the way to the right. Notice how all of the flashing
squares have red outlines around them indicating they are returning error messages.

image::rollouts-demo-errors.png[]

[IMPORTANT]
This section should not be overly timing sensitive since we are
cheating a bit by pre-generating errors. However when we promote the new image the error
setting will be reset back to 0 for the preview service. After the promotion pipeline completes,
immediately switch back to the tab with the preview service and push the error bar
back to 100%.

Now we will perform a promotion to a yellow image, go back to the OpenShift Pipeline and start it
with the yellow image:

image::console-pipeline-promote-yellow.png[]

As mentioned in the note above, once the pipeline is finished you must immediately switch back
to the preview service and set the error rate to 100%.

If you did everything correctly, you should see that the Analysis failed as indicated by the red
button:

image::argo-rollouts-analysis-failed-details.png[]

[NOTE]
If the timing did not work out for you, you can try again by promoting with a different color.
You can see the list of available colors .link:https://quay.io/repository/openshiftdemos/rollouts-demo?tab=tags[here, window='_blank']
where each color is simply a tag in our quay.io registry.

If you expand the Analysis section, you will see at least one metric test has failed as
pictured above. You can hover the mouse over the failed analysis to see the details.

Examining the rollout with the CLI using the previous `get` command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc argo rollouts get rollout rollouts-demo -n user%USERNUM%-prod
----

Will now show the following:

[.console-output]
[source,bash,subs="attributes+,+macros"]
----
Name:            rollouts-demo
Namespace:       user%USERNUM%-prod
Status:          ✖ Degraded
Message:         RolloutAborted: Rollout aborted update to revision 3: Metric "success-rate" assessed Failed due to failed (1) > failureLimit (0)
Strategy:        BlueGreen
Images:          quay.io/openshiftdemos/rollouts-demo:green (stable, active)
Replicas:
  Desired:       2
  Current:       2
  Updated:       0
  Ready:         2
  Available:     2

NAME                                                        KIND         STATUS        AGE    INFO
⟳ rollouts-demo                                             Rollout      ✖ Degraded    23m
├──# revision:3
│  ├──⧉ rollouts-demo-6b8dccb648                            ReplicaSet   • ScaledDown  8m46s  preview,delay:passed
│  └──α rollouts-demo-6b8dccb648-3-pre                      AnalysisRun  ✖ Failed      8m43s  ✔ 1,✖ 1
│     └──⊞ d484ff69-4ef1-4cf1-9a10-1b78530dadc7.run-load.1  Job          ✔ Successful  8m43s
├──# revision:2
│  ├──⧉ rollouts-demo-5999df6cf9                            ReplicaSet   ✔ Healthy     18m    stable,active
│  │  ├──□ rollouts-demo-5999df6cf9-cw5bm                   Pod          ✔ Running     18m    ready:1/1
│  │  └──□ rollouts-demo-5999df6cf9-rfgzr                   Pod          ✔ Running     18m    ready:1/1
│  └──α rollouts-demo-5999df6cf9-2-pre                      AnalysisRun  ✔ Successful  18m    ✔ 5
│     └──⊞ 17525abd-96e7-4abe-bac3-e9f62249a353.run-load.1  Job          ✔ Successful  18m
└──# revision:1
   └──⧉ rollouts-demo-66d84bcd76                            ReplicaSet   • ScaledDown  23m
----

[#cleanup]
== Clean-up

Prior to moving on to the next module we need to perform some clean-up activities. First let's reset the Development environment back to blue:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc apply -k ./deploy/base -n user%USERNUM%-dev
----

Next we will delete the Rollout in the `user%USERNUM%-prod` so we can start fresh as we explore the canary strategy next.

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
kustomize build ./bluegreen-analysis/base | sed "s/%SUB_DOMAIN%/$SUB_DOMAIN/" | sed "s/%USER%/%USERNUM%/" | oc delete -n user%USERNUM%-prod -f -
----
